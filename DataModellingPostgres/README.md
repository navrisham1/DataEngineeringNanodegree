# Data Modelling with Postgres

This project consists of three modules:
- Data modeling with Postgres using  psycopg2 adapter. Implementing a star schema.
- Develop ETL processes for each table.
- Building an ETL pipeline using Python to process the entire dataset.

## Introduction

Sparkify wants to analyze the songs and user activity data which is collected  on their music streaming app. 
Currently, there is no easy way to query the data, which resides in a directory in form of JSON data.
The analytics team is particularly interested in understanding what songs users are listening to. 

Task is to create a database schema and ETL pipeline for this analysis.

## DataSets
- Song dataset: The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. 

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- Log dataset:The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

```
{"artist":"Slipknot","auth":"Logged In","firstName":"Aiden","gender":"M","itemInSession":0,"lastName":"Ramirez","length":192.57424,"level":"paid","location":"New York-Newark-Jersey City, NY-NJ-PA","method":"PUT","page":"NextSong","registration":1540283578796.0,"sessionId":19,"song":"Opium Of The People (Album Version)","status":200,"ts":1541639510796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"20"}
```

##  Schema Design
The schema used for this exercise is the Star Schema.There is one main fact table and 4 dimensional tables.

Benefits of using relational database for this case:
 
 - Data is systematically stored and modeled in tabular format.
- A standard query language like SQL  can be used to access the data .
- We can use JOINS to answer business related queries.

#### Fact Table

1.  **songplays**  - records in log data associated with song plays i.e. records with page  `NextSong`
    -   _songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent_

#### Dimension Tables

2.  **users**  - users in the app
    -   _user_id, first_name, last_name, gender, level_
3.  **songs**  - songs in music database
    -   _song_id, title, artist_id, year, duration_
4.  **artists**  - artists in music database
    -   _artist_id, name, location, latitude, longitude_
5.  **time**  - timestamps of records in  **songplays**  broken down into specific units
    -   _start_time, hour, day, week, month, year, weekday_



## Project files

Files used on the project:
1. **data** folder nested at the home of the project, where all json files reside.
2. **sql_queries.py** contains all  sql queries, and is imported into the files bellow.
3. **create_tables.py** drops and creates tables. Run this file to reset  tables before each time you run ETL scripts.
4. **test.ipynb** displays the first few rows of each table that enables to check the database.
5. **etl.ipynb** reads and processes a single file from song_data and log_data and loads the data into tables. 
6. **etl.py** reads and processes files from song_data and log_data and loads them into tables. 
7. **README.md** current file, provides discussion on the project.

## Project steps
- Create Tables

1.  Write  `CREATE`  statements in  `sql_queries.py` to create each table.
2.  Write  `DROP`  statements in  `sql_queries.py`  to drop each table if it exists.
3.  Run  `create_tables.py`in terminal   to create  database and tables.
4.  Run  `test.ipynb`  to confirm the creation of tables with the correct columns. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.

- Build ETL Processes

In etl.ipynb develop ETL processes for each table. At the end of each table section, or at the end of the notebook, run  `test.ipynb`  to confirm that records were successfully inserted into each table. 
Remember to rerun  `create_tables.py`  to reset tables before each time we run this notebook.

- Build ETL Pipeline

`etl.py`pipeline is built for  processing  the entire data set. Remember to run  `create_tables.py`  before running  `etl.py`  to reset  tables. Run  `test.ipynb`  to confirm that records were successfully inserted into each table.

